{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1080, 1, 2, 750)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dataloader as dl\n",
    "\n",
    "# dataSource\n",
    "td, tl, pd, pl = dl.read_bci_data()\n",
    "td.shape    # numbers of shape represent (N, C, H, W)/(batch size, channels, height, weight)\n",
    "# td.len  # batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1080, 1, 2, 750])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "trainset = TensorDataset(torch.from_numpy(td), torch.from_numpy(tl))\n",
    "# trainset.tensors[0].shape\n",
    "\n",
    "trainloader = DataLoader(dataset=trainset, batch_size=64, shuffle=False)\n",
    "trainloader.dataset.tensors[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network 訓練步驟\n",
    "1. 訓練Model\n",
    "1. 計算Loss (MSE、CrossEntropy)\n",
    "1. 最佳化Model (Optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGNET(nn.Module):\n",
    "  def __init__(self, actFun) -> None:\n",
    "    super(EEGNET, self).__init__()\n",
    "    match actFun:\n",
    "      case \"ELU\":\n",
    "        self.activation = nn.ELU(alpha=1.0)\n",
    "      case \"ReLU\":\n",
    "        self.activation = nn.ReLU()\n",
    "      case \"LeakyReLU\":\n",
    "        self.activation = nn.LeakyReLU()\n",
    "\n",
    "    # Layer 1\n",
    "    self.FirstConv = nn.Sequential(\n",
    "      nn.Conv2d(1, 16, kernel_size=(1, 51), stride=(1, 1), padding=(0, 25), bias=False)\n",
    "      , nn.BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    )\n",
    "    \n",
    "    # Layer 2\n",
    "    self.DepthWiseConv = nn.Sequential(\n",
    "      nn.Conv2d(16, 32, kernel_size=(2, 1), stride=(1, 1), groups=16, bias=False)\n",
    "      , nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      , nn.AvgPool2d(kernel_size=(1, 4), stride=(1, 4), padding=0)\n",
    "      , self.activation\n",
    "      , nn.Dropout(p=0.25)\n",
    "    )\n",
    "\n",
    "    # Layer 3\n",
    "    self.SeperableConv = nn.Sequential(\n",
    "      nn.Conv2d(32, 32, kernel_size=(1, 15), stride=(1, 1), padding=(0, 7), bias=False)\n",
    "      , nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      , self.activation\n",
    "      , nn.AvgPool2d(kernel_size=(1, 8), stride=(1, 8), padding=0)\n",
    "      , nn.Dropout(p=0.25)\n",
    "    )\n",
    "\n",
    "    self.Classify = nn.Sequential(\n",
    "      nn.Linear(in_features=736, out_features=2, bias=True)\n",
    "    )\n",
    "  \n",
    "  def forward(self, x):  # 直接寫model(input)就等於call forward這個函數了\n",
    "    x = self.FirstConv(x)\n",
    "    x = self.DepthWiseConv(x)\n",
    "    x = self.SeperableConv(x)\n",
    "\n",
    "    x = x.view(-1, 736) # reshape to fit the classifier (-1部分讓python自己推測)\n",
    "    x = self.Classify(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "# EEGNET model架構\n",
    "# model = EEGNET(\"ELU\")\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note\n",
    "* model相關\n",
    "    * **model只做forwrad**\n",
    "    * model可以呼叫`model.train()`來將model變成訓練模式；呼叫`model.eval()`則會變成預測模式\n",
    "    * model繼承nn.Module後可以直接用`model(input)`來執行forward，但記得自己的model中還是需要有forward這個函數\n",
    "    * model預設的輸入值是double，可以藉由`model.float()`更換model parameter型態\n",
    "* loss function相關\n",
    "    * **loss負責做backword(計算gradient)**\n",
    "    * `nn.CrossEntropyLoss(output, target)` -> target型態必須是Long\n",
    "    * `loss.item()`就是loss的值\n",
    "* optimizer相關\n",
    "    * **optimizer負責update weights -> `optimizer.step()`**\n",
    "    * 在新的forward開始之前要先把之前的gradient清掉 -> optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, model, batch_size, learning_rate, epochs) -> None:\n",
    "        self.model = model.double()\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.loss_function = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    def Train(self, train_data, train_label):\n",
    "        # train mode(告訴model現在要開始訓練了)\n",
    "        self.model.train()\n",
    "\n",
    "        for ep in range(self.epochs):\n",
    "            print(\"Epoch {}\".format(ep))\n",
    "            cost = 0.0\n",
    "\n",
    "            for i in range(len(train_data)//self.batch_size):\n",
    "                self.optimizer.zero_grad()  # 清空上次的gradient\n",
    "\n",
    "                output = self.model(torch.from_numpy(train_data[i:i+32]).double())    # forwarding\n",
    "                label = torch.from_numpy(train_label[i:i+32])\n",
    "                loss = self.loss_function(output, label.long())    # nn.CrossEntropyLoss(predict_val, label)\n",
    "                loss.backward() # calculate gradient\n",
    "\n",
    "                self.optimizer.step()   # update weights\n",
    "                cost += loss.item()\n",
    "            print(\"loss = {}\".format(loss))\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "loss = 0.12208653556210528\n",
      "Epoch 1\n",
      "loss = 0.060115851060044974\n",
      "Epoch 2\n",
      "loss = 0.03188135619752519\n",
      "Epoch 3\n",
      "loss = 0.0036024826119408506\n",
      "Epoch 4\n",
      "loss = 0.0012098700069008315\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 1e-2\n",
    "epochs = 5\n",
    "activation_fun = \"ELU\"\n",
    "\n",
    "trainer = ModelTrainer(EEGNET(activation_fun), batch_size, learning_rate, epochs)\n",
    "trainer.Train(td, tl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
